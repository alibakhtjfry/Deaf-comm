

# DeafComm

**Real-Time Speech â†” Sign Language Communication System**

DeafComm is an AI-driven communication platform developed to support **instant, two-way interaction between deaf/mute individuals and hearing users**.
The system enables communication by translating **sign language into text** and converting **spoken or written language into sign language using a 3D animated avatar**.

The project addresses accessibility challenges by combining **computer vision, deep learning, natural language processing, and mobile application development** into a unified assistive solution.

---

## ğŸ‘¨â€ğŸ’» Project Team & Responsibilities

| Name                                   | Registration No. | Role                        | Key Contributions                                                                                                                |
| -------------------------------------- | ---------------- | --------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| **Rafay Ahmad Hussain (Group Leader)** | BCSF22M014       | Research & AI Development   | Literature review, research gap identification, dataset handling, sign-to-text pipeline, avatar-based sign generation, subtitles |
| **Ali Bakht Jafry**                    | BCSF22M044       | UI & System Integration     | User interface design, dataset evaluation, system integration, deployment                                                        |
| **M. Sheharyar Khan Daym**             | BCSF22M046       | Model Training & Backend    | Model training, testing and validation, backend services, system deployment                                                      |
| **Umer Anwar**                         | BCSF22M049       | Avatar & Dataset Management | Avatar coordination, annotation tools, dataset preparation and labeling                                                          |

> Work was divided according to a structured Work Breakdown Structure (WBS)
> and executed collaboratively using an iterative development approach.

---

## ğŸŒ± Repository Structure

The repository is organized using a **modular design philosophy** to ensure clarity, scalability, and ease of maintenance.

* `main` â€“ Consolidated and stable implementation
* Supporting directories â€“ AI models, avatar system, backend services, and mobile UI components

This structure allows independent module development and seamless integration.

---

## ğŸ” System Integration Overview

The system consists of interconnected modules that collectively enable real-time communication:

* Sign language recognition and text generation
* Speech/text processing and sign language synthesis
* Avatar-based sign visualization
* Backend APIs for communication between models and mobile interface

Each module operates independently while contributing to the complete communication pipeline.

---

## ğŸ§  Core System Features

* âœ” Real-time sign language to text translation
* âœ” Speech and text to sign language conversion
* âœ” Deep learning-based gesture recognition
* âœ” Progressive transformer-based motion prediction
* âœ” 3D avatar visualization for sign output
* âœ” MediaPipe-based hand landmark extraction
* âœ” Backend API layer for model integration
* âœ” Mobile-friendly user interface
* âœ” Performance evaluation using accuracy, SER, and latency metrics

The system supports **bidirectional communication**, enabling meaningful interaction for both user groups.

---

## ğŸ› ï¸ Technologies Used

* **Programming Language:** Python
* **Machine Learning Framework:** PyTorch
* **Gesture Processing:** MediaPipe (Hand Landmarks)
* **Text-to-Sign Translation:** Transformer Encoderâ€“Decoder Architecture
* **Avatar Representation:** Skeletal Joint Motion Regression
* **Backend:** FastAPI
* **Mobile Application:** React Native
* **Configuration Management:** YAML
* **Evaluation Metrics:** Accuracy, Sign Error Rate (SER), Mean Squared Error (MSE), Dynamic Time Warping (DTW)

---

## ğŸ“‹ Development Methodology

* Requirements gathered through **literature review and stakeholder analysis**
* Functional and non-functional requirements documented using **RTM**
* System design guided by **Agile SDLC principles**
* Iterative development with continuous testing and validation
* Performance-focused optimization for mobile execution

---


## ğŸ“ Additional Notes

* The system primarily focuses on **Pakistan Sign Language (PSL)**
* Designed as an **assistive technology**, not a replacement for professional interpreters
* Emphasis on **low latency, usability, and accessibility**
* Architecture supports future extension to additional sign languages

---

## ğŸ“§ Contact

For academic, technical, or evaluation-related queries regarding this project, please contact any member of the project team listed above.

---

ğŸŒ **DeafComm â€” enabling inclusive communication through intelligent technology.**

---
